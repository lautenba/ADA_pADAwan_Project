{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import locale\n",
    "#from html.parser import HTMLParser\n",
    "# import website_func.py to use its functions\n",
    "from website_func import *\n",
    "import os #To read the file\n",
    "#reload every module each time\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some quick analysis of the data\n",
    "We received a data set of .html file containing the website content of recipes.\n",
    "We wanted to sort them by website, in order to, more easily, find a pattern among them. This will enable us to do the \"scraping\" of the pages. First we thought about moving the files in a folder corresponding to their website, but it would be a waste of time and a big computational effort. Thus, we came up with a (probably) faster solution : we could simply write the name of the file within its corresponding website folder. By inspecting the files, we saw that the first line was always containing a comment with the name of the file and the complete website. Using readlines and split, we could easily retrieve the name of the website.\n",
    "\n",
    "We launched this process, but an alarm appeared describing a Trojan virus in the file \"1c2cb6f0df04cf5a9d0baa116c6aa7bb.html\". \n",
    "We had then to quarantine or mabye remove the file, as we have quite enough\n",
    "By doing so, we remarked the file \"msg.log\" that could help us into fastering the processus as its content is formed of the name of the file together with its website. [SHOW] We obserbed that occasionally a line containing other info that the content we want can appear. We need to ignore this\n",
    "Also we noticed there are no file extensions other than .html and .log. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'recipePages.zip\\\\recipePages\\\\msg.log'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-010748f193b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mretrieve_website_from_log\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\OneDrive\\Documents\\EPFL\\Master 3\\Applied data analysis\\Lab\\project\\ADA_pADAwan_Project\\website_func.py\u001b[0m in \u001b[0;36mretrieve_website_from_log\u001b[1;34m()\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[0mpath_to_log\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"recipePages.zip/recipePages/msg.log\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_to_log\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[0mweb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwebsite_from_log\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'recipePages.zip\\\\recipePages\\\\msg.log'"
     ]
    }
   ],
   "source": [
    "retrieve_website_from_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recipePages\\msg.log\n"
     ]
    }
   ],
   "source": [
    "# We can find in the folder that, excepted the html files, there is only the log\n",
    "\n",
    "# Get all filenames (i.e. path) that are in recipePages folder \n",
    "pathlist = Path(\"recipePages/\").glob('**/*')\n",
    "i = 0\n",
    "for path in pathlist:\n",
    "     # because path is object not string\n",
    "    path_in_str = str(path)\n",
    "    if not path_in_str.endswith(\".html\"):\n",
    "        print(path_in_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "website_count=pd.Series()\n",
    "\n",
    "for (root,dirs,files) in os.walk('SortedFiles', topdown=\"True\"):   \n",
    "        for website in dirs:\n",
    "            \n",
    "            filename = \"SortedFiles/\" + website +\"/filesName.txt\"\n",
    "            \n",
    "            f = open(filename, \"r\")\n",
    "            number_of_link = len(f.readlines())\n",
    "            line_to_add=pd.Series(number_of_link,index=[website])\n",
    "            website_count = website_count.append(line_to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "website_count.sort_values(ascending=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "website_count_used=website_count[website_count.values>1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "website_list_used=website_count_used.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['allrecipes.com', 'food.com', 'foodnetwork.com', 'yummly.com', 'cooks.com', 'epicurious.com', 'tasteofhome.com', 'myrecipes.com', 'recipes.sparkpeople.com', 'cdkitchen.com', 'bettycrocker.com', 'cookeatshare.com', 'southernfood.about.com', 'grouprecipes.com', 'recipe.com', 'kraftrecipes.com', 'eatingwell.com', 'ifood.tv', 'delish.com']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recipePagesT/000a3333ad24828769b6be5a5e1bdb4a.html\n",
      "Chicken Breast Cutlets with Artichokes and Capers\n",
      "4.4\n",
      "Reviews : 80\n",
      "<class 'int'>\n",
      "\n",
      "\n",
      "recipePagesT/000b861ad15679c578d81884a87689ea.html\n",
      "Best Ever Popcorn Balls\n",
      "4.4\n",
      "Reviews : 322\n",
      "<class 'int'>\n",
      "\n",
      "\n",
      "recipePagesT/00a149d30ee1176043ed750ac51b4fee.html\n",
      "Orange Cream Cheese Frosting\n",
      "4.6\n",
      "Reviews : 39\n",
      "<class 'float'>\n",
      "\n",
      "\n",
      "recipePagesT/00a1d99a6383cdeac3d6c253f8e7cff1.html\n",
      "We don't care about this page\n",
      "recipePagesT/00a405ea8c3d491b677a995cb558b99f.html\n",
      "Perfect Baked Potato\n",
      "4.7\n",
      "Reviews : 410\n",
      "<class 'int'>\n",
      "\n",
      "\n",
      "recipePagesT/00a50320c5b99874292e760601993c21.html\n",
      "Pumpkin Oatmeal\n",
      "3.9\n",
      "Reviews : 88\n",
      "<class 'int'>\n",
      "\n",
      "\n",
      "recipePagesT/00a53bf43b5779e20f2ec5bfca29dda4.html\n",
      "Baked Asparagus with Balsamic Butter Sauce\n",
      "4.6\n",
      "Reviews : 1319\n",
      "<class 'int'>\n",
      "\n",
      "\n",
      "recipePagesT/00a58140b06973b9ac0da04b4b10f67e.html\n",
      "Chicken Corn Chowder\n",
      "4.5\n",
      "Reviews : 42\n",
      "<class 'int'>\n",
      "\n",
      "\n",
      "recipePagesT/00a7450a98369de9097e042172e59fa4.html\n",
      "We don't care about this page\n",
      "recipePagesT/00ab5078eaeeb0bcec9284b8dad89ea7.html\n",
      "Gluten-Free Cheese and Herb Pizza Crust\n",
      "4.2\n",
      "Reviews : 12\n",
      "<class 'int'>\n",
      "\n",
      "\n",
      "recipePagesT/00adfe6c1d097676993b9ba951a6fb1e.html\n",
      "Fry Bread I\n",
      "4.3\n",
      "Reviews : 81\n",
      "<class 'int'>\n",
      "\n",
      "\n",
      "recipePagesT/00b77a83b90fcd9ec7270fe9c6387596.html\n",
      "Authentic Korean Bulgogi\n",
      "4.7\n",
      "Reviews : 15\n",
      "<class 'int'>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for (root,dirs,files) in os.walk('recipePagesT', topdown=\"True\"):  \n",
    "    for webpage in files:\n",
    "        filename= 'recipePagesT/'+webpage\n",
    "        \n",
    "        #Try to open it in a simple way to avoid expensive calculation\n",
    "        try:\n",
    "            f = open(filename,'r')\n",
    "            first_line=f.readline()\n",
    "            f.close()\n",
    "        except:\n",
    "            continue\n",
    "            #If we can't open it in a simple way go do it with Beautifulsoup\n",
    "            #try:\n",
    "            #    tree = html.parse(filename)\n",
    "            #    website = html.tostring(tree)  \n",
    "            #    i=1;\n",
    "            #except:\n",
    "            #    print(\"We aren't able to read a document called:\" + wesite)\n",
    "            #    continue\n",
    "        website = str(first_line).split(\"/\")[2].strip(\"www.\")\n",
    "        \n",
    "        #Determine if we have to spare the data\n",
    "        if website in website_list_used :\n",
    "            \n",
    "            #Read the data from the first website\n",
    "            if website in website_list_used[0]:\n",
    "                print(filename)\n",
    "                soup = BeautifulSoup(open(filename), \"html.parser\")\n",
    "                \n",
    "                recipe_name = soup.find('span', class_='itemreviewed')\n",
    "                \n",
    "                #We only take recipe and not searching pages\n",
    "                if recipe_name is None:\n",
    "                    print(\"We don't care about this page\")\n",
    "                    continue\n",
    "                recipe_name = recipe_name.text\n",
    "                rating_html = soup.find('img', class_='rating')\n",
    "                \n",
    "                #Determine the postion\n",
    "                start_rank = 59\n",
    "                end_rank = 62\n",
    "                rating = rating_html['title'][start_rank:end_rank]\n",
    "                \n",
    "                print(recipe_name)\n",
    "                print(rating)\n",
    "                \n",
    "                #Determine the number of reviews\n",
    "                review_html = soup.find('span', class_='count').text\n",
    "                review = int(review_html.replace(',',''))\n",
    "                print('Reviews :',review)\n",
    "                \n",
    "                #Time to prepare\n",
    "                prepare_time = np.nan\n",
    "                soup1 = soup.find('span', class_='totalTime')\n",
    "                if soup1 is not None:\n",
    "                    \n",
    "                    prepare_time_html = soup1.find('span', class_='value-title')\n",
    "                    #Determine the postion\n",
    "                    start_prepare_time = 2\n",
    "                    end_prepare_time = len(prepare_time_html['title']) \n",
    "                    prepare_time_not_converted = prepare_time_html['title'][start_prepare_time:end_prepare_time]\n",
    "                    time_analyse = prepare_time_not_converted.split('H')\n",
    "                    \n",
    "                    #We only have minutes\n",
    "                    if len(time_analyse) == 1:\n",
    "                        prepare_time = int(time_analyse[0].replace('M',''))\n",
    "                    if len(time_analyse) == 2:\n",
    "                        time_minute = time_analyse[1].replace('M','');\n",
    "                        prepare_time = int(time_analyse[0])*60 + (int(time_minute) if time_minute else 0) \n",
    "                    \n",
    "                    \n",
    "                \n",
    "                print(type(prepare_time))\n",
    "                print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(soup1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada] *",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
